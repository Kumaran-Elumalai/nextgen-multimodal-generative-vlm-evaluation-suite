# ðŸ§ ðŸ“¸ Visionâ€“Language Models (VLMs) for Visual Question Answering

Modern AI systems are increasingly **multimodal**, capable of processing and generating information from diverse sources such as **images and text**.  
This project focuses on **Visionâ€“Language Models (VLMs)** designed to combine visual and textual understanding for:

- **Visual Question Answering (VQA)**
- **Descriptive image generation and captioning**

## ðŸš€ Overview

This repository demonstrates practical multimodal AI development by implementing two advanced VLMs:

### ðŸ”¹ ViLT â€“ Vision-and-Language Transformer
- Lightweight and optimized for CPU-friendly deployment  
- Fast inference  
- Ideal for **single-word or short-answer VQA**

### ðŸ”¹ SmolVLM â€“ Small Vision-Language Model
- Generates multi-sentence, descriptive responses  
- Great for **detailed image understanding and explanation**
