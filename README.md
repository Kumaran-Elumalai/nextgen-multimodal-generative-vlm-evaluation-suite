# ğŸ§ ğŸ“¸ Visionâ€“Language Models (VLMs) for Visual Question Answering

Modern AI systems are increasingly **multimodal**, capable of processing and generating information from diverse sources such as **images and text**.  
This project focuses on **Visionâ€“Language Models (VLMs)** designed to combine visual and textual understanding for:

- **Visual Question Answering (VQA)**
- **Descriptive image generation and captioning**

## ğŸš€ Overview

This repository demonstrates practical multimodal AI development by implementing two advanced VLMs:

### ğŸ”¹ ViLT â€“ Vision-and-Language Transformer
- Lightweight and optimized for CPU-friendly deployment  
- Fast inference  
- Ideal for **single-word or short-answer VQA**

### ğŸ”¹ SmolVLM â€“ Small Vision-Language Model
- Generates multi-sentence, descriptive responses  
- Great for **detailed image understanding and explanation**

## ğŸ¯ Project Goals

This project aims to provide:

- âœ”ï¸ **End-to-end multimodal pipelines** for image + text reasoning  
- âœ”ï¸ **Efficient CPU-based inference** without requiring GPUs  
- âœ”ï¸ **Clean, professional, interactive AI demos** showcasing real-world VQA and multimodal capabilities  

## ğŸ“ Repository Structure

```
nextgen-multimodal-generative-vlm-evaluation-suite/
â”‚â”€â”€ vilt_vqa/
â”‚   â””â”€â”€ vilt_app.py
â”‚â”€â”€ smolvlm_vqa/
â”‚   â””â”€â”€ smolvlm_app.py
â”‚â”€â”€ assets/
â”‚   â””â”€â”€ sample_images/
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
```
## ğŸ§  Features

- **Two Multimodal Pipelines:**  
  ViLT (classification-style VQA) and SmolVLM (generative, descriptive answers)

- **Interactive Gradio Interfaces:**  
  Upload an image and get a professional, descriptive answer

- **CPU-Friendly Implementation:**  
  Optimized for environments without GPU

## ğŸ”§ Technical Highlights

### ğŸ”¹ Visionâ€“Language Processing
- **ViLT:** Uses attention-based fusion between image patches and text tokens  
- **SmolVLM:** Combines a Vision Transformer encoder with an LLM decoder for generative outputs

### ğŸ”¹ Prompt Engineering
- Designed to produce descriptive, multi-sentence answers  
- Formatting ensures structured and readable responses

### ğŸ”¹ Inference Optimizations
- Image resizing and normalization  
- Controlled token generation (48â€“60 tokens) to balance latency and richness

### ğŸ”¹ Why These Models?
- **ViLT â†’** Efficient, lightweight transformer ideal for fact-based VQA  
- **SmolVLM â†’** Produces rich, detailed multi-sentence explanations

## ğŸš€ Usage Instructions
- Run ViLT App
```
cd vilt_vqa
python vilt_app.py
```

- Run SmolVLM App
```
cd smolvlm_vqa
python smolvlm_app.py
```
#### A Gradio interface will open in your browser automatically.

## ğŸ“Š Multimodal Model Comparison Summary

### 1. **Model Architecture**

| Model    | Parameters | Architecture                          | Fusion Strategy               |
|----------|------------|----------------------------------------|-------------------------------|
| **ViLT** | ~200M      | Vision Transformer + Text Embeddings   | Early fusion (non-generative) |
| **SmolVLM** | ~500M   | Vision Encoder + Language Decoder      | Cross-modal generative fusion |

---

### 2. **Capabilities**

| Model    | Output                          | Strengths                    | Limitations                         |
|----------|----------------------------------|------------------------------|--------------------------------------|
| **ViLT** | Single-word / short phrase       | Lightweight, CPU-friendly    | Cannot produce descriptive sentences |
| **SmolVLM** | Multi-sentence generative      | Rich, coherent descriptions  | Extremely slow on CPU                |

---

### 3. **Performance (CPU Inference) â€” Updated**

| Model    | Avg. CPU Time  | RAM Usage     | Notes                                     |
|----------|----------------|---------------|-------------------------------------------|
| **ViLT** | 10â€“12 seconds  | ~2â€“3 GB       | Fast enough for real-time VQA             |
| **SmolVLM** | 5â€“6 minutes | ~4â€“6 GB       | Generative decoding is slow on CPU        |

---

### 4. **Use-Case Fit**

| Scenario               | Best Model | Reason                           |
|------------------------|-----------|-----------------------------------|
| Quick factual VQA      | ViLT      | Fast, lightweight                 |
| Scene description       | SmolVLM   | Strong generative ability         |
| Low compute environments| ViLT      | Minimal latency                   |
| Rich multimodal understanding | SmolVLM | Decoder-based generative power |


