# ğŸ§ ğŸ“¸ Visionâ€“Language Models (VLMs) for Visual Question Answering

Modern AI systems are increasingly **multimodal**, capable of processing and generating information from diverse sources such as **images and text**.  
This project focuses on **Visionâ€“Language Models (VLMs)** designed to combine visual and textual understanding for:

- **Visual Question Answering (VQA)**
- **Descriptive image generation and captioning**

## ğŸš€ Overview

This repository demonstrates practical multimodal AI development by implementing two advanced VLMs:

### ğŸ”¹ ViLT â€“ Vision-and-Language Transformer
- Lightweight and optimized for CPU-friendly deployment  
- Fast inference  
- Ideal for **single-word or short-answer VQA**

### ğŸ”¹ SmolVLM â€“ Small Vision-Language Model
- Generates multi-sentence, descriptive responses  
- Great for **detailed image understanding and explanation**

## ğŸ¯ Project Goals

This project aims to provide:

- âœ”ï¸ **End-to-end multimodal pipelines** for image + text reasoning  
- âœ”ï¸ **Efficient CPU-based inference** without requiring GPUs  
- âœ”ï¸ **Clean, professional, interactive AI demos** showcasing real-world VQA and multimodal capabilities  

## ğŸ“ Repository Structure

nextgen-multimodal-generative-vlm-evaluation-suite/
â”‚â”€â”€ vilt_vqa/
â”‚ â””â”€â”€ vilt_app.py
â”‚â”€â”€ smolvlm_vqa/
â”‚ â””â”€â”€ smolvlm_app.py
â”‚â”€â”€ assets/
â”‚ â””â”€â”€ sample_images/
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
